{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from utils.ML_helper import cargar_videos_y_etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function utils.ML_helper.cargar_videos_y_etiquetas(ruta_base, clases, num_frames=30)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cargar_videos_y_etiquetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = ['Cometa', 'Ecuador_celeste', 'Meteoroide', 'Meteoro', 'Zodiaco']\n",
    "ruta_base = \"Dataset\"\n",
    "\n",
    "# Upload the dataset\n",
    "imagenes, etiquetas = cargar_videos_y_etiquetas(ruta_base, clases)\n",
    "x_train, x_test, y_train, y_test = train_test_split(imagenes, etiquetas, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\UdeA\\Aprendizaje-estadistico\\LSC_Astronophysics\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, TimeDistributed, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Definir la arquitectura CNN + LSTM\n",
    "modelo = Sequential([\n",
    "    # Extraer características con CNN\n",
    "    TimeDistributed(Conv2D(32, (3, 3), activation='relu', input_shape=(None, 200, 200, 3))),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(64, (3, 3), activation='relu')),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(128, (3, 3), activation='relu')),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "\n",
    "    TimeDistributed(Conv2D(256, (3, 3), activation='relu')),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "\n",
    "    TimeDistributed(Flatten()),  # Aplanar cada frame\n",
    "\n",
    "    # LSTM para aprender la relación entre los frames\n",
    "    LSTM(128, return_sequences=False),  \n",
    "\n",
    "    # Capa totalmente conectada\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # Capa de salida\n",
    "    Dense(len(clases), activation='softmax')\n",
    "])\n",
    "\n",
    "modelo.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 91s/step - accuracy: 0.2500 - loss: 1.6094 - val_accuracy: 0.0000e+00 - val_loss: 1.6110\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 56s/step - accuracy: 0.2500 - loss: 1.6090 - val_accuracy: 0.0000e+00 - val_loss: 1.6126\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 60s/step - accuracy: 0.2500 - loss: 1.6086 - val_accuracy: 0.0000e+00 - val_loss: 1.6142\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 51s/step - accuracy: 0.2500 - loss: 1.6082 - val_accuracy: 0.0000e+00 - val_loss: 1.6158\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 52s/step - accuracy: 0.2500 - loss: 1.6078 - val_accuracy: 0.0000e+00 - val_loss: 1.6174\n",
      "Epoch 6/10\n"
     ]
    }
   ],
   "source": [
    "modelo.fit(x_train, y_train, epochs=10, batch_size=8, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "modelo = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(clases), activation='softmax')\n",
    "])\n",
    "\n",
    "modelo.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "modelo.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test), callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = modelo.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class indices\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=clases, yticklabels=clases)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and metrics\n",
    "resultados = modelo.evaluate(x_test, y_test)\n",
    "print(f\"Precisión: {resultados[1] * 100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "modelo.save('modelo_lsc2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from utils.ML_helper import cargar_datos_y_etiquetas\n",
    "\n",
    "# Define dataset path and classes\n",
    "ruta_base = \"Dataset\"\n",
    "clases = ['Cometa', 'Ecuador_celeste', 'Meteoroide', 'Meteoro', 'Zodiaco']\n",
    "\n",
    "# Load dataset\n",
    "imagenes, etiquetas = cargar_datos_y_etiquetas(ruta_base, clases)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(imagenes, etiquetas, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure y_train is a 1D integer array\n",
    "y_train_flat = np.ravel(y_train).astype(int)\n",
    "\n",
    "# Compute Class Weights to Balance Dataset\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(y_train_flat), \n",
    "    y=y_train_flat\n",
    ")\n",
    "class_weights_dict = {i: class_weights[i] for i in np.unique(y_train_flat)}\n",
    "\n",
    "# Define CNN Model\n",
    "modelo = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(200, 200, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(clases), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "modelo.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "modelo.fit(\n",
    "    x_train, y_train_flat,\n",
    "    epochs=30,  # Increased epochs for better learning\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test),\n",
    "    class_weight=class_weights_dict,  # Balanced class training\n",
    "    callbacks=[tensorboard]\n",
    ")\n",
    "\n",
    "# Save the Model\n",
    "modelo.save('modelo_lsc3.h5')\n",
    "\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "modelo.save('modelo_lsc3.h5')\n",
    "\n",
    "print(\"Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale for test set\n",
    "\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=32)\n",
    "test_generator = test_datagen.flow(x_test, y_test, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "modelo = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(200, 200, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(clases), activation='softmax')\n",
    "])\n",
    "\n",
    "modelo.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_train is a 1D integer array with the correct length\n",
    "y_train_flat = np.array(y_train).reshape(-1)  # Ensure it's 1D and has the correct length\n",
    "print(\"Expected samples:\", x_train.shape[0])\n",
    "print(\"Actual y_train samples:\", y_train_flat.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Ensure y_train is a 1D integer array\n",
    "y_train_flat = np.ravel(y_train).astype(int)  # Convert to 1D and ensure integer labels\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
    "\n",
    "# Convert to dictionary format required by model.fit()\n",
    "class_weights_dict = {cls: weight for cls, weight in zip(np.unique(y_train_flat), class_weights)}\n",
    "\n",
    "# Train the model using class weights\n",
    "modelo.fit(x_train, y_train_flat, epochs=10, batch_size=32, validation_data=(x_test, y_test),\n",
    "           class_weight=class_weights_dict, callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Real Time Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"modelo_lsc2.h5\")\n",
    "\n",
    "# Define class labels (Ensure this matches your training classes)\n",
    "clases = ['Cometa', 'Ecuador_celeste', 'Meteoroide', 'Meteoro', 'Zodiaco']  # Update this with your classes\n",
    "\n",
    "# Initialize MediaPipe for hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Start webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB and process hand detection\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    predictions = []  # Store predictions for multiple hands\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get bounding box around the hand\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = int(min([landmark.x for landmark in hand_landmarks.landmark]) * w)\n",
    "            x_max = int(max([landmark.x for landmark in hand_landmarks.landmark]) * w)\n",
    "            y_min = int(min([landmark.y for landmark in hand_landmarks.landmark]) * h)\n",
    "            y_max = int(max([landmark.y for landmark in hand_landmarks.landmark]) * h)\n",
    "\n",
    "            # Extract and preprocess hand image\n",
    "            hand_img = frame[y_min:y_max, x_min:x_max]\n",
    "            if hand_img.size > 0:\n",
    "                hand_img = cv2.resize(hand_img, (200, 200))\n",
    "                hand_img = hand_img.astype('float32') / 255.0  # Normalize\n",
    "                hand_img = np.expand_dims(hand_img, axis=0)  # Add batch dimension\n",
    "\n",
    "                # Predict the class\n",
    "                prediction = model.predict(hand_img)\n",
    "                class_index = np.argmax(prediction)\n",
    "                predictions.append(clases[class_index])\n",
    "\n",
    "    # Display only the most confident prediction\n",
    "    if predictions:\n",
    "        most_common_prediction = max(set(predictions), key=predictions.count)\n",
    "        cv2.putText(frame, f\"Prediction: {most_common_prediction}\", (50, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the webcam feed with predictions\n",
    "    cv2.imshow(\"Real-Time Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"modelo_lsc3.h5\")\n",
    "\n",
    "# Define class labels (Ensure this matches your training classes)\n",
    "clases = ['Cometa', 'Ecuador_celeste', 'Meteoroide', 'Meteoro', 'Zodiaco']  # Update this with your classes\n",
    "\n",
    "# Initialize MediaPipe for hand tracking\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Load the video file\n",
    "video_path = \"Dataset/Meteoroide/14.mp4\"  # Change this to your uploaded video filename\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Output processed video\n",
    "output_path = \"output_video.mp4\"\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB and process hand detection\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    predictions = []  # Store predictions for multiple hands\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        x_min, y_min, x_max, y_max = frame.shape[1], frame.shape[0], 0, 0\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get bounding box around all detected hands\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = min(x_min, int(min([landmark.x for landmark in hand_landmarks.landmark]) * w))\n",
    "            x_max = max(x_max, int(max([landmark.x for landmark in hand_landmarks.landmark]) * w))\n",
    "            y_min = min(y_min, int(min([landmark.y for landmark in hand_landmarks.landmark]) * h))\n",
    "            y_max = max(y_max, int(max([landmark.y for landmark in hand_landmarks.landmark]) * h))\n",
    "\n",
    "        # Extract and preprocess hand image\n",
    "        hand_img = frame[y_min:y_max, x_min:x_max]\n",
    "        if hand_img.size > 0:\n",
    "            hand_img = cv2.resize(hand_img, (200, 200))\n",
    "            hand_img = hand_img.astype('float32') / 255.0  # Normalize\n",
    "            hand_img = np.expand_dims(hand_img, axis=0)  # Add batch dimension\n",
    "\n",
    "            # Predict the class\n",
    "            prediction = model.predict(hand_img)\n",
    "            class_index = np.argmax(prediction)\n",
    "            predicted_label = clases[class_index]\n",
    "\n",
    "            # Display the prediction\n",
    "            cv2.putText(frame, f\"Prediction: {predicted_label}\", (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Write the processed frame to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Show the processed video frame\n",
    "    cv2.imshow(\"Video Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Processed video saved as {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
